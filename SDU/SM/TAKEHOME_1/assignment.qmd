---
title: "Take Home Assignment 1"
author: "Jan Ryszkiewicz"
format: pdf
mainfont: Latin Modern Roman
sansfont: Latin Modern Roman
---

# Task 1

### subtask 1

We perform basic inspection:

```{r}
load("Data ST523 813 E2025 Exam.rdata")
df <- Data
pairs(df)
nrow(df)
ncol(df)
summary(df)
```

We fit a default model:

```{r}
model = lm(CO ~ AT + AP + AH + GTEP + TIT + TAT + CDP + TEY, data = df)
```

### subtask 2

Display all the estimated parameters:

```{r}
coef(model)
```

And the one for ambient temperature in particular:
```{r}
coef(model)["AT"]
```

From this we know that the estimated change in CO for a 1°C increase in ambient temperature, other predictors constant, is -0.0216 units.

### subtask 3

Perform F-Test:

```{r}
summary(model)
```

As we can see:

`F-statistic: 102.8 on 8 and 291 DF,  p-value: < 2.2e-16`

P-value is very low which indicates the rejection of $H_0$ - global null hypotesis at significance $\alpha = 0.05$

We have 291 residual degrees of freedom (n - p) and 8 model degrees of freedom (p - 1)

Value of the F-test statistic is 102.8

### subtask 4

Let's start by fitting 2 submodels:
```{r}
M_A = lm(CO ~ AT + AP + AH, data = df)
M_B = lm(CO ~ GTEP + TIT + TAT + TEY, data = df)
```

Now try comparing ambient only model to the default:

```{r}
anova(M_A, model)
```

As we can see by the corresponding F-statistic ( 146.4 ) and p-value ( < 2.2e-16)
The default model explains the data *much better* i.e.
$$
\exists{_{i\ne AT, AP, AH}} \text{ such that } \beta_i \ne 0 
$$
So we reject the $H_0$ that all other predictiors aside from *AT,AP,AH* are $= 0$.


What about the process only model?:

```{r}
anova(M_B, model)
```

The rejection of the null hypothesis—that the reduced (process-only) model explains the data equally well—is supported by the F statistic (~2.58) and the corresponding p-value (0.037). This suggests that, at a significance level of $\alpha = 0.05$, adding other predictors leads to a statistically significant improvement in model fit. However, for $\alpha = 0.01$, the evidence would not be strong enough to reject the null hypothesis.

Therefore we conclude that we *can reduce* the default model to the Process Only, however only when accepting our significance
level to be $< 0.04$

### subtask 5

From the last subtask we can conclude that M_B is better than M_A therefore M_B is chosen as our final model for this subtask.

By inspecting adjusted $R^2$ between models we can see the diffrence of total adjusted explained variation between models.

```{r}
R_Ambient = summary(M_A)$adj.r.squared
R_Process = summary(M_B)$adj.r.squared
R_default = summary(model)$adj.r.squared

R_Ambient

R_Process

R_default

```

The absolute reduction of unexplained variation for Process Only model M_B:

```{r}
explained_variation_Model_B = summary(M_B)$r.squared
CO = df$CO
TSS = sum((CO - mean(CO))^2)
RSS = sum(residuals(M_B)^2)
absolute_reduction_Model_B = TSS - RSS

explained_variation_Model_B

absolute_reduction_Model_B 
```

Let us introduce another alternative model M_Alt that includes CDP as one of its predictors:

```{r}
M_Alt = lm(CO ~ GTEP + TIT + TAT + CDP + TEY, data = df)

anova(M_Alt,M_B)
```

Analyzing this anova table we can conclude that the alternative model is better as the perforemd partial F test returned low p'value

However we could further reduce the M_Alt:

```{r}
anova(M_Alt)
```

Here we see that the F test corresponding to the inclusion of TEY predictor shows a relatively high p'value that might indicate corresonding $\beta_{TEY} \approx 0$

It is worth performing $R^2$ inspection for the alternate model M_Alt that includes CDP predictor and removes TEY as it seemed redundant:

```{r}
M_Alt = lm(CO ~ GTEP + TIT + TAT + CDP, data = df)

explained_variation_ModelAlt = summary(M_Alt)$r.squared
RSS = sum(residuals(M_Alt)^2)
absolute_reduction_ModelAlt = TSS - RSS

explained_variation_ModelAlt

absolute_reduction_ModelAlt
```

The absolute reduction in the residual sum of squares is only slightly higher for model M_Alt, at the cost of including one additional predictor *(CDP)*, and removing one other *(TEY)*. Depending on our circumstances, we can choose to either stay by *M_Alt* for marginally better performance or opt for the different *M_B*, which performs almost equivalently.


# Task 2
### subtask 1
The model presented in task can also be written as:
$$
\quad
Y =
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_{45}
\end{bmatrix},
\quad
X =
\begin{bmatrix}
1 & \mathbb{1}_{2,1} & \mathbb{1}_{3,1} & \mathbb{1}_{4,1} & X_1 \\
1 & \mathbb{1}_{2,2} & \mathbb{1}_{3,2} & \mathbb{1}_{4,2} & X_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & \mathbb{1}_{2,45} & \mathbb{1}_{3,45} & \mathbb{1}_{4,45} & X_45 \\
\end{bmatrix},
\quad
\beta^T =
\begin{bmatrix}
\mu \\
\alpha_2 \\
\alpha_3 \\
\alpha_4 \\
b
\end{bmatrix},
\quad
\epsilon =
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_{45}
\end{bmatrix}.
$$

$$
\mathbb{1}_{i,j} =
\begin{cases}
1 & \text{if } \alpha_j \text{ present in sample } i \\
0 & \text{otherwise}
\end{cases}
$$

Where column $1$ of $X$ corresponds to always present $\mu$ and each column $2,3,4$ is filled with $0, 1$ depending on corresponding $\alpha_{j(i)}$ (indicator function)

Also $\alpha_i$ defined as:
\begin{align*}
\alpha_2 &= \text{Temporary – Research/Academic (relative to Permanent)} \\
\alpha_3 &= \text{Temporary – Private Consultant (relative to Permanent)} \\
\alpha_4 &= \text{Freelance (relative to Permanent)}
\end{align*}

We know that:
$$
\begin{aligned}
\text{Cov}(\hat{\alpha}_2, \hat{\alpha}_3) &= 22{,}000{,}000 \\
\text{Cov}(\hat{\alpha}_2, \hat{\alpha}_4) &= 20{,}000{,}000 \\
\text{Cov}(\hat{\alpha}_3, \hat{\alpha}_4) &= 21{,}000{,}000
\end{aligned}
$$

And:

$$
\begin{aligned}
\hat{\text{SE}}(\hat\mu) &= 20{,}000\\
\hat{\text{SE}}(\hat{\alpha}_2) &= 24{,}000\\
\hat{\text{SE}}(\hat{\alpha}_3) &= 23{,}000\\
\hat{\text{SE}}(\hat{\alpha}_4) &= 22{,}000\\
\end{aligned}
$$

First we want to calculate the *CL* for $\alpha_2 - \alpha_3$ with confidence 90%

Which is:

$$
\begin{aligned}
c &= [0, 1, -1, 0, 0], \\
\hat{\psi} &= c^T\hat{\beta} = \hat{\alpha}_2 - \hat{\alpha}_3, \\
\hat{\text{SE}}(\hat{\psi}) &= \sqrt{cVar(\hat{\beta})c^T},\\
&=\sqrt{Var(\hat{\alpha}_2 - \hat{\alpha}_3)}\\
&= \sqrt{Var(\hat{\alpha}_2) + Var(\hat{\alpha}_3) - 2Cov(\hat{\alpha}_2, \hat{\alpha}_3)}, \\
\text{CI}_{90\%} &= \hat{\psi} \pm t_{45-5,\, 1-0.10/2} \cdot \hat{\text{SE}}(\hat{\psi})
\end{aligned}
$$

That then by substitution becomes:
$$
\begin{aligned}
\hat{\psi} &= -40{,}000 - (- 10{,}000) \\
&= -30{,}000\\
\hat{\text{SE}}(\hat{\psi}) &= \sqrt{(24{,}000)^2 + (23{,}000)^2 - 2 \cdot 22{,}000{,}000} \\
&\approx  32{,}573\\
t_{40,\, {(1-0.10)/2}} &\approx 1.68385\\
\text{CI}_{90\%} &= \hat{\psi} \pm t_{45-5,\, 1-0.10/2} \cdot \hat{\text{SE}}(\hat{\psi})\\
\text{CI}_{90\%} &= -30{,}000 \pm  1.68385 \cdot 32{,}573 \\
&= -30{,}000 \pm 54848
\end{aligned}
$$

Therefore the Confidence interval is:
$$
\begin{aligned}
\text{CI}_{90\%} &= [-84{,}848 \text{ , } 24{,}848 ] 
\end{aligned}
$$

That concludes the first subtask of Task 2.

### subtask 2

Next we want to look for statistical evidence for:
$\alpha_2  \le \alpha_3$

Therefore we perform One Sided hypotesis Test on:

$$
\begin{aligned}
H_0 = \alpha_2  - \alpha_3 > 0 \\
H_a = \alpha_2  - \alpha_3 \le 0 \\
\end{aligned}
$$

$H_0$ - temporary researchers are earning **more** than temporary private consultants

$H_a$ - the opposite


We are using the same $\hat{\psi}$ from previous subtask:
$$
\begin{aligned}
\hat{\psi} &= \hat{\alpha}_2 - \hat{\alpha}_3 \\
T &= \frac{(\hat{\alpha}_2 - \hat{\alpha}_3) - 0}{\hat{\text{SE}}(\hat{\psi})}\\
&= \frac{-30{,}000}{32{,}573}\\
&\approx -0.921
\end{aligned}
$$

Then:
```{r}
qt(0.05, df = 40)
```

$$ 
\begin{aligned}
-0.921 &> -1.683851 \\
T &> t_{40,\, 0.05} \\ 
\end{aligned}
$$

**We cannot reject  $H_0$**

Therefore there is not enough statistical evidence that temporary researchers are earning less
than temporary private consultants.



# Task 3

We have $x_i \in [-2,2]$ for $i = 1, \dots, n$.
The $X$ and $\beta$ matrces are as follows:
$$
X =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix},
\quad
\beta =
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}.
$$
Therefore, the variance of $\hat{\beta}$ is
$$
\text{Var}(\hat{\beta}) = \sigma^2 (X^\top X)^{-1} = 
\sigma^2 
\begin{bmatrix}
n & \sum_i x_i \\
\sum_i x_i & \sum_i x_i^2
\end{bmatrix}^{-1}.
$$
Following the inverse this becomes
$$
\text{Var}(\hat{\beta}) =
\frac{\sigma^2}{n \sum_i x_i^2 - (\sum_i x_i)^2} 
\begin{bmatrix}
\sum_i x_i^2 & - \sum_i x_i \\
- \sum_i x_i & n
\end{bmatrix}.
$$
And then, variance of the slope estimate ($\hat{\beta_1}$) is:
$$
\text{Var}(\hat{\beta_1}) =
\frac{\sigma^2n}{n \sum_i x_i^2 - (\sum_i x_i)^2} 
$$
Which further reduces to:
$$
\text{Var}(\hat{\beta}_1) =
\frac{\sigma^2}{ \sum_i x_i^2 - \frac{1}{n}(\sum_i x_i)^2} =
\frac{\sigma^2}{ \sum_i x_i^2 - \frac{2}{n}(\sum_i x_i)^2 + \frac{1}{n}(\sum_i x_i)^2} = 
$$
$$
= \frac{\sigma^2}{ \sum_i x_i^2 - 2\sum_i x_i \cdot \bar{x} + n\bar{x}^2} = 
\frac{\sigma^2}{ \sum_i (x_i - \bar{x})^2 }
$$
We want to minimize the $\text{Var}(\hat{\beta}_1)$.
Which can be done by maximizing $\sum_i (x_i - \bar{x})^2$ as $\sigma^2$ is constant.
This can be achieved by spreading $x_1, \dots, x_n$ as much as possible that will ideally
set $\bar{x} = \frac{1}{n}\sum_i(x_i) = 0$

We know that: 
$$
x_i \in [-2, 2], \quad \forall i = 1, \dots, n
$$
Then we can choose:

- $x_1, \dots, x_{n/2} = -2$  
- $x_{n/2+1}, \dots, x_n = 2$

By that we minimize $\text{Var}(\hat{\beta}_1)$ now equal to:
$$
\text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{ \sum_i (x_i - \bar{x})^2 } = 
\frac{\sigma^2}{\sum_i(\pm 2 - 0)^2} = \frac{\sigma^2}{4n}
$$
And we achive maximum possible precission

Q.E.D.

# Task 4

Only for Master's
