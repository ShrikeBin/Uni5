---
title: "Take Home Assignment 1"
author: "Jan Ryszkiewicz"
format: pdf
---

# Task 1

### subtask 1

We perform basic inspection:

```{r}
load("Data ST523 813 E2025 Exam.rdata")
df <- Data
pairs(df)
nrow(df)
ncol(df)
summary(df)
```

We fit a default model:

```{r}
model = lm(CO ~ AT + AP + AH + GTEP + TIT + TAT + CDP + TEY, data = df)
```

### subtask 2

Display all the estimated parameters:

```{r}
coef(model)
```

And the one for ambient temperature in particular:
```{r}
coef(model)["AT"]
```

From this we know that the estimated change in CO for a 1°C increase in ambient temperature, other predictors constant, is -0.0216 units.

### subtask 3

Perform F-Test:

```{r}
summary(model)
```

As we can see:

`F-statistic: 102.8 on 8 and 291 DF,  p-value: < 2.2e-16`

P-value is very low which indicates the rejection of $H_0$ - global null hypotesis at significance $\alpha = 0.05$

We have 291 residual degrees of freedom (n - p) and 8 model degrees of freedom (p - 1)

Value of the F-test statistic is 102.8

### subtask 4

Let's start by fitting 2 submodels:
```{r}
M_A = lm(CO ~ AT + AP + AH, data = df)
M_B = lm(CO ~ GTEP + TIT + TAT + CDP + TEY, data = df)
```

Now try comparing ambient only model to the default:

```{r}
anova(M_A, model)
```

As we can see by the corresponding F-statistic ( 146.4 ) and p-value ( < 2.2e-16)
The default model explains the data *much better*

What about the process only model?:

```{r}
anova(M_B, model)
```

Here we can see, that rejection of the null hypotesis being that default model explains the data better is not possible, as the corresponding F statistic (~ 1.76) and p-value ( 0.15 ) suggest the redundancy of ambient variables in the fitting
i. e. adding *AT + AP + AH* predictors to Process Only fit does **not** significantly improve the model.

Therefore we conclude that we *can reduce* the default model to the Process Only one as it does not significantly worsen the model.

### subtask 5

First should we examine our current fit:

```{r}
anova(M_B)
```

We notice that **Anova**'s chain of hypotesis testing table indicates the possibility fitting an alternative model, as including TEY as predictor appears unnecessary:

```{r}
M_Alt = lm(CO ~ GTEP + TIT + TAT + CDP, data = df)
anova(M_Alt, M_B)
```

Here, in the F test we can see that p value is not lower than $\alpha = 0.05$ therefore reducing the number of variables again doesnt seem to worsen the model - we can use the alternative one from now on.

By inspecting adjusted $R^2$ between models we can see that the jump between our alternative model and the one proposed in the last subtask is not that significant.

```{r}
R_Ambient = summary(M_A)$adj.r.squared
R_Alt = summary(M_Alt)$adj.r.squared
R_Process = summary(M_B)$adj.r.squared
R_default = summary(model)$adj.r.squared

R_Ambient

R_Alt

R_Process

R_default

```

The final inspection of variation let's choose Process Only model M_B:

```{r}
explained_variation = summary(M_B)$r.squared
CO = df$CO
TSS = sum((CO - mean(CO))^2)
RSS = sum(residuals(M_B)^2)
absolute_reduction = TSS - RSS

explained_variation

absolute_reduction
```

It is worth performing similar inspection for the alternate model M_Alt:

```{r}
explained_variation = summary(M_Alt)$r.squared
RSS = sum(residuals(M_Alt)^2)
absolute_reduction = TSS - RSS

explained_variation

absolute_reduction
```

The absolute reduction in the residual sum of squares is only slightly higher for model M_B, at the cost of including one additional predictor. Depending on the available computational power, we can choose to either stay by M_B for marginally better performance or opt for the slightly lighter M_Alt, which performs almost equivalently.


# Task 2
### subtask 1
The model presented in task can also be written as:
$$
X =
\begin{bmatrix}
1 & 0 & 0 & 0 & X_1 \\
1 & 0 & 0 & 0 & X_2 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 1 & 0 & 0 & X_{_2} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 1 & 0 & X_{_3} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 0 & 1 & X_{_4} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & 0 & 0 & 0 & X_{45}
\end{bmatrix},
\quad
\beta =
\begin{bmatrix}
\mu \\
\alpha_2 \\
\alpha_3 \\
\alpha_4 \\
b
\end{bmatrix},
\quad
Y =
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_{45}
\end{bmatrix}.
$$
Where each column $2,3,4$ in $X$ is filled with $0, 1$ depending on corresponding $\alpha_{i(j)}$
And column $1$ corresponds to always present $\mu$

Also $\alpha_i$ defined as:
\begin{align*}
\alpha_2 &= \text{Temporary – Research/Academic (relative to Permanent)} \\
\alpha_3 &= \text{Temporary – Private Consultant (relative to Permanent)} \\
\alpha_4 &= \text{Freelance (relative to Permanent)}
\end{align*}

We know that:
$$
\begin{aligned}
\text{Cov}(\hat{\alpha}_2, \hat{\alpha}_3) &= 22{,}000{,}000 \\
\text{Cov}(\hat{\alpha}_2, \hat{\alpha}_4) &= 20{,}000{,}000 \\
\text{Cov}(\hat{\alpha}_3, \hat{\alpha}_4) &= 21{,}000{,}000
\end{aligned}
$$

And:

$$
\begin{aligned}
\hat{\text{SE}}(\hat\mu) &= 20{,}000\\
\hat{\text{SE}}(\hat{\alpha}_2) &= 24{,}000\\
\hat{\text{SE}}(\hat{\alpha}_3) &= 23{,}000\\
\hat{\text{SE}}(\hat{\alpha}_4) &= 22{,}000\\
\end{aligned}
$$

First we want to calculate the *CL* for $\alpha_2 - \alpha_3$ with confidence 90%

Which is:

$$
\begin{aligned}
c &= [0, 1, -1, 0, 0], \\
\hat{\psi} &= c^T\hat{\beta} = \hat{\alpha}_2 - \hat{\alpha}_3, \\
\hat{\text{SE}}(\hat{\psi}) &= \sqrt{cVar(\hat{\beta})c^T},\\
&=\sqrt{Var(\hat{\alpha}_2 - \hat{\alpha}_3)}\\
&= \sqrt{Var(\hat{\alpha}_2) + Var(\hat{\alpha}_3) - 2Cov(\hat{\alpha}_2, \hat{\alpha}_3)}, \\
\text{CI}_{90\%} &= \hat{\psi} \pm t_{45-5,\, 1-0.10/2} \cdot \hat{\text{SE}}(\hat{\psi})
\end{aligned}
$$

That then by substitution becomes:
$$
\begin{aligned}
\hat{\psi} &= -40{,}000 - (- 10{,}000) \\
&= -30{,}000\\
\hat{\text{SE}}(\hat{\psi}) &= \sqrt{(24{,}000)^2 + (23{,}000)^2 - 2 \cdot 22{,}000{,}000} \\
&\approx  32{,}573\\
t_{40,\, {(1-0.10)/2}} &\approx 1.68385\\
\text{CI}_{90\%} &= \hat{\psi} \pm t_{45-5,\, 1-0.10/2} \cdot \hat{\text{SE}}(\hat{\psi})\\
\text{CI}_{90\%} &= -30{,}000 \pm  1.68385 \cdot 32{,}573 \\
&= -30{,}000 \pm 54848
\end{aligned}
$$

Therefore the Confidence interval is:
$$
\begin{aligned}
\text{CI}_{90\%} &= [-84{,}848 \text{ , } 24{,}848 ] 
\end{aligned}
$$

That concludes the first subtask of Task 2.

### subtask 2

Next we want to look for statistical evidence for:
$\alpha_2  \le \alpha_3$

Therefore we perform One Sided hypotesis Test on:

$$
\begin{aligned}
H_0 = \alpha_2  - \alpha_3 > 0 \\
H_a = \alpha_2  - \alpha_3 \le 0 \\
\end{aligned}
$$

$H_0$ - temporary researchers are earning **more** than temporary private consultants

$H_a$ - the opposite


We are using the same $\hat{\psi}$ from previous subtask:
$$
\begin{aligned}
\hat{\psi} &= \hat{\alpha}_2 - \hat{\alpha}_3 \\
T &= \frac{(\hat{\alpha}_2 - \hat{\alpha}_3) - 0}{\hat{\text{SE}}(\hat{\psi})}\\
&= \frac{-30{,}000}{32{,}573}\\
&\approx -0.921
\end{aligned}
$$

Then:
```{r}
qt(0.05, df = 40)
```

$$ 
\begin{aligned}
-0.921 &> -1.683851 \\
T &> t_{40,\, 0.05} \\ 
\end{aligned}
$$

**We cannot reject  $H_0$**

Therefore there is not enough statistical evidence that temporary researchers are earning less
than temporary private consultants.



# Task 3

We have $x_i \in [-2,2]$ for $i = 1, \dots, n$.
The $X$ and $\beta$ matrces are as follows:
$$
X =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix},
\quad
\beta =
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}.
$$
Therefore, the variance of $\hat{\beta}$ is
$$
\text{Var}(\hat{\beta}) = \sigma^2 (X^\top X)^{-1} = 
\sigma^2 
\begin{bmatrix}
n & \sum_i x_i \\
\sum_i x_i & \sum_i x_i^2
\end{bmatrix}^{-1}.
$$
Following the inverse this becomes
$$
\text{Var}(\hat{\beta}) =
\frac{\sigma^2}{n \sum_i x_i^2 - (\sum_i x_i)^2} 
\begin{bmatrix}
\sum_i x_i^2 & - \sum_i x_i \\
- \sum_i x_i & n
\end{bmatrix}.
$$
And then, variance of the slope estimate ($\hat{\beta_1}$) is:
$$
\text{Var}(\hat{\beta_1}) =
\frac{\sigma^2n}{n \sum_i x_i^2 - (\sum_i x_i)^2} 
$$
Which further reduces to:
$$
\text{Var}(\hat{\beta}_1) =
\frac{\sigma^2}{ \sum_i x_i^2 - \frac{1}{n}(\sum_i x_i)^2} =
\frac{\sigma^2}{ \sum_i x_i^2 - \frac{2}{n}(\sum_i x_i)^2 + \frac{1}{n}(\sum_i x_i)^2} = 
$$
$$
= \frac{\sigma^2}{ \sum_i x_i^2 - 2\sum_i x_i \cdot \bar{x} + n\bar{x}^2} = 
\frac{\sigma^2}{ \sum_i (x_i - \bar{x})^2 }
$$
We want to minimize the $\text{Var}(\hat{\beta}_1)$.
Which can be done by maximizing $\sum_i (x_i - \bar{x})^2$ as $\sigma^2$ is constant.
This can be achieved by spreading $x_1, \dots, x_n$ as much as possible that will ideally
set $\bar{x} = \frac{1}{n}\sum_i(x_i) = 0$

We know that: 
$$
x_i \in [-2, 2], \quad \forall i = 1, \dots, n
$$
Then we can choose:

- $x_1, \dots, x_{n/2} = -2$  
- $x_{n/2+1}, \dots, x_n = 2$

By that we minimize $\text{Var}(\hat{\beta}_1)$ now equal to:
$$
\text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{ \sum_i (x_i - \bar{x})^2 } = 
\frac{\sigma^2}{\sum_i(\pm 2 - 0)^2} = \frac{\sigma^2}{4n}
$$
And we achive maximum possible precission

Q.E.D.

# Task 4

Only for Master's
