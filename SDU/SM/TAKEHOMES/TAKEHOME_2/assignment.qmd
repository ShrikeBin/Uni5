---
format:
    pdf:
        include-in-header: "template_answers.tex"
mainfont: STIX Two Text
sansfont: STIX Two Text
mathfont: Latin Modern Math
---
\maketitle

\upOne
# Task 1

## subtask 1

The dataset contains 113 observations across 9 variables. Summary statistics are presented below:

\upHalf
\codePrintStart
```{r, fig.width=18, fig.height=18}
#| echo: false
#| message: false
#| warning: false
load("Data_Assignment2_Ex1_E2025.rdata")
df <- cgm_data
print(summary(df[, sapply(df, is.numeric)]))
```
\codePrintEnd

For the adhesive, as it is a factor variable, we only show the overall amount of data entries per type:

\upHalf
\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
print(table(df$adhesive_type))
```
\codePrintEnd
\nextpart

Relationship with Lifetime plots:

![](plots/Lifetime_vs_activity_level.png){width=50%}![](plots/Lifetime_vs_adhesive_type.png){width=50%}
![](plots/Lifetime_vs_calibration_error.png){width=50%}![](plots/Lifetime_vs_experience.png){width=50%}
![](plots/Lifetime_vs_humidity.png){width=50%}![](plots/Lifetime_vs_patient_bmi.png){width=50%}
![](plots/Lifetime_vs_skin_temp.png){width=50%}![](plots/Lifetime_vs_sweat_rate.png){width=50%}

It is worth noting that on the plots we can see 4 points which significantly higher lifetime stands out from the rest

\nexttask

## subtask 2

Since lifetime is strictly positive and continuous, I considered two GLM families:

- Gamma distribution with log and canonical (inverse) links

- Inverse Gaussian distribution with log link

Log link function is chosen as the relationship between lifetime and humidity seems exponential - other continuous predictors appear linear on the plots.
The canonical link for Inverse Gaussian caused convergence issues, so it was excluded.

I do not consider Poisson distribution as it is designed for count data, not continuous responses. 
Binomial on the other hand is mainly used for proportion data which is not present in our example.

We compare candidate models based on AIC:

\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
df$adhesive_type <- as.factor(df$adhesive_type)

gamma_model_log <- glm(lifetime ~ skin_temp + humidity + activity_level +
                     sweat_rate + calibration_error + patient_bmi +
                     experience + adhesive_type,
                   data = df,
                   family = Gamma(link = "log"))
gamma_model_canon <- glm(lifetime ~ skin_temp + humidity + activity_level +
                         sweat_rate + calibration_error + patient_bmi +
                         experience + adhesive_type,
                       data = df,
                       family = Gamma(link = "inverse"))
inv_gauss_model_log <- glm(lifetime ~ skin_temp + humidity + activity_level +
                         sweat_rate + calibration_error + patient_bmi +
                         experience + adhesive_type,
                       data = df,
                       family = inverse.gaussian(link = "log"))
models <- list(
  Gamma_Log = gamma_model_log,
  Gamma_Canonical= gamma_model_canon,
  Inv_Gaussian_Log = inv_gauss_model_log
)

aic_values <- sapply(models, AIC)
print(sort(aic_values))
```
\codePrintEnd
\upHalf
and residual deviance:

\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
resid_dev <- sapply(models, function(m) if("deviance" %in% names(m)) m$deviance else NA)
print(sort(resid_dev))
```
\codePrintEnd

The Gamma distribution with log linking was selected based on lowest AIC. Even though Inverse Gaussian had slightly lower residual deviance, the substantial AIC difference ($\Delta \approx 80$) and the excessive cubic mean-variance relationship ($\mu^3$) were not necessary given the visually observed variance structure on the plots.
It is worth bearing in mind the existance of cathegorical predictor in the model - `adhesive_type` - during the model selectio the p-value is low enough to consider it statistically significant.

Full fit model can be tested for interaction using `(...)^2` syntax, I performed sensible pairwise tests for the predictors including their interaction terms, ie. relating `activity_level` to, for instance, `patient_bmi`, `sweat_rate` and `humidity`, relating `experience:humidity` or `experience:calibration_error` etc, yielded no statistically significant interaction terms between the predictors (their p-values were > 0.05). **However when testing individual interaction between each predictor and `adhesive_type` I noticed the existence of an interaction between `patient_bmi` and the aformentioned `adhesive_type`, what motivates the inclusion of this term in my model.**


\nextpart

We consider 2 different predictor selection methods:

- Stepwise, based on AIC using `step()`, on default model with included `patient_bmi` interaction:
```{r}
#| message: false
#| warning: false
Model_1 <- step(glm(lifetime ~ skin_temp + humidity + activity_level +
                     sweat_rate + calibration_error +
                     experience + patient_bmi * adhesive_type,
                   data = df,
                   family = Gamma(link = "log")), trace=0)
```

- P-value based selection (threshold: p < 0.05):

\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
round(summary(glm(lifetime ~ skin_temp + humidity + activity_level +
                     sweat_rate + calibration_error + 
                     experience + patient_bmi * adhesive_type,
                   data = df,
                   family = Gamma(link = "log")))$coefficients, 5)
Model_2 <- glm(lifetime ~ skin_temp + humidity + activity_level +
                     calibration_error + patient_bmi * adhesive_type,
                   data = df,
                   family = Gamma(link = "log"))
final_model = Model_2
# summary(alt_best_model)
```
\codePrintEnd

Initial full model coefficients were deemed:

- Significant (p < 0.05): humidity, calibration_error, skin_temp, activity_level, patient_bmi
- Non-significant : sweat_rate (p=0.635), experience (p=0.426),
- Adhesive type C showed significance by interaction with bmi during testing, therefore all cathegorical predictors of `adhesive_type` retained.

\nextpart

Comparison between two models:

- Model 1 - step() best.
- Model 2 - p-value based choice.

Is not needed as after the stepwise model selection based on Akaike Information Criterion and selection based on p values we end up with the same model, different in one predictor.

\nexttask

## subtask 3

Final model is chosen to be Gamma GLM with log link with following predictors:

\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
round(summary(final_model)$coefficients, 4)
```
\codePrintEnd

\nextpart

Since experience is not in the final model, it does not not affect the prediction. Using patient_bmi = 30, activity_level=4 and sample means for remaining predictors (`adhesive_type` = A as reference):

\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
mean_skin_temp <- mean(df$skin_temp)
mean_humidity <- mean(df$humidity)
mean_cal_error <- mean(df$calibration_error)

pred <- coef(final_model)["(Intercept)"] +
       coef(final_model)["skin_temp"] * mean_skin_temp +
       coef(final_model)["humidity"] * mean_humidity +
       coef(final_model)["activity_level"] * 4 +
       coef(final_model)["calibration_error"] * mean_cal_error +
       coef(final_model)["patient_bmi"] * 30
       
print(exp(pred))
```
\codePrintEnd
calculated by:

\upHalf
$$ 
\begin{aligned}
&\hat{\mu} = \exp(\beta_0 + \beta_1 \cdot x_\text{skin temp} + \beta_2 \cdot x_\text{humidity} + \beta_3 \cdot x_\text{activity level} + \beta_4 \cdot x_\text{calibration error} + \beta_5 \cdot x_\text{patient bmi} + ...) \\
\end{aligned}
$$
Where "..." normally represent the `adhesive_type` and interaction terms between `patien_bmi` and `adhesive_type`, however in our case they are all set to 0, because we use adhesive type A, which is the reference category in R (and also the most common type in the data).

Since experience was removed from the final model, changing experience by 1 year produces no significant change in expected lifetime.

\nextpart

Plots presented here deliberatly cut off the lifetime >15 outlier points (only 4 of them) to increase visibility.
As experience is not included as predictor in our final model following plot is made using default, full-fit model including all predictors.

![](plots/AUX_Predicted_Lifetime_vs_Experience.png){width=60% fig-align="center"}

As we can see, user experience does not seem to impact the liftime in a meaningfull way compared to our chosen predictors, which effect is clearly visible on the following plots:

![](plots/Predicted_Lifetime_vs_Activity.png){width=50%}![](plots/Predicted_Lifetime_vs_CalibrationError.png){width=50%}
![](plots/Predicted_Lifetime_vs_Humidity.png){width=50%}![](plots/Predicted_Lifetime_vs_SkinTemp.png){width=50%}
![](plots/Predicted_Lifetime_vs_AdhesvieAndBMI.png){width=80% fig_align="center"}

Here we can clearly see why the inclusion of the interaction term is necessary.
The effect of adhesive type on sensor lifetime varies with patient BMI, as indicated by the differing slopes of the predicted curves.

\nextpart

Final Model Deviance:
\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
deviance(final_model)
```
\codePrintEnd
\upHalf
Final Model AIC:
\codePrintStart
```{r}
#| echo: false
#| message: false
#| warning: false
AIC(final_model)
```
\codePrintEnd
\nextpart

Final Model's Dispersion parameter is:
\codePrintStart
```{r}
#| message: false
#| warning: false
summary(final_model)$dispersion
```
\codePrintEnd

This indicates underdispersion. While less problematic than overdispersion, this suggests the variance is smaller than expected under the Gamma model. The model fit quality seems enough, though a potential quasi-Gamma approach could be considered if precision is critical.

\nextpart

Sensor lifetime is significantly influenced by environmental and technical factors: higher skin temperature and higher humidity increases lifetime, while higher calibration_error and activity_level decrease it. Humidity shows the strongest effect, followed by calibration_error, while skin temperature and activity level have more modest impacts. The data points strongly towards an interaction between `patien_bmi` and `adhesive_type` meaning that their specific combination influences the results in a different way. User experience, along with their sweat rate, do not significantly impact sensor lifetime and were excluded from the final model.

\nexttask

# Task 2

Because our transformation $g_\alpha$ is monotonically increasing (trivial checks for $y \geq 0$ and $y < 0$)
$$
P(Y_i < y_i) = P(g_\alpha(Y_i) < g_\alpha(y_i))
$$

And since $g_\alpha(Y_i)$ is normally distributed, we can use the standard normal CDF $\Phi$ with normalized $Y_i$ with its mean and standard error:

$$
P(Y_i < y_i) = \Phi\left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \beta}{\sigma}\right)
$$

Taking the derivative with respect to $y_i$ gives us:

$$
\frac{d}{dy_i} P(Y_i < y_i) = \phi\left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \beta}{\sigma}\right) \cdot \frac{\dot{g}_\alpha(y_i)}{\sigma}
$$

where $\phi$ is the standard normal density and $\dot{g}_\alpha(y_i)$ being the derivative of $g_\alpha(y_i)$ defined as:

$$
\dot{g}_\alpha(y_i) =
\begin{cases}
(y_i + 1)^{\alpha - 1}, & y \ge 0 \\[6pt]
(1- y_i)^{1 - \alpha}, & y < 0
\end{cases}
$$

The final density of $Y_i$ is:

$$
\begin{aligned}
f_{Y_i}(y_i) =& \phi\left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \beta}{\sigma}\right) \cdot \frac{\dot{g}_\alpha(y_i)}{\sigma} = \\
=&\frac{1}{\sqrt{2\pi}}
\exp\Bigg\{ -\frac{1}{2} \left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \beta}{\sigma}\right)^2 \Bigg\} 
\cdot \frac{\dot{g}_\alpha(y_i)}{\sigma}
\end{aligned}
$$

\nextpart

$$
\ell(\alpha, \beta, \sigma^2; y_1, \dots, y_n) 
= \sum_{i=1}^{n} \log f_{Y_i}(y_i)
$$

For independent $Y_i$ with density $f_{Y_i}$, this becomes:

$$
\ell(\alpha, \beta, \sigma^2; y_1, \dots, y_n)
= \log \Bigg[
\prod_{i=1}^{n} 
\frac{1}{\sqrt{2\pi}} 
\exp\Bigg\{ -\frac{1}{2} \left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \beta}{\sigma}\right)^2 \Bigg\} 
\cdot \frac{\dot{g}_\alpha(y_i)}{\sigma}
\Bigg]
$$

\nextpart

The estimates derived from equating the derivative of loglikelihood with respect to $\beta$ and $\sigma$ to 0 (as we did on the lecture) are:

$$
\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top g_\alpha(\mathbf{Y})
$$

$$
\hat{\sigma}^2 = \frac{1}{n} RSS_g
$$

Where
$$
RSS_g = \sum_{i=1}^{n} \Big( g_\alpha(y_i) - \mathbf{x}_i^\top \hat{\beta} \Big)^2 
$$

\nextpart

$$
l(\alpha; y_1, \dots, y_n) 
= 
\log \Bigg[
\prod_{i=1}^{n} 
\frac{1}{\sqrt{2\pi}} 
\exp\Bigg\{ -\frac{1}{2} \left(\frac{g_\alpha(y_i) - \mathbf{x}_i^\top \hat{\beta}}{\hat{\sigma}}\right)^2 \Bigg\} 
\cdot \frac{\dot{g}_\alpha(y_i)}{\hat{\sigma}}
\Bigg]
$$

Which equals to:

$$
= -\frac{n}{2} \log(2\pi) - \frac{1}{2 \hat{\sigma}^2} \sum_{i=1}^{n} \Big( g_\alpha(y_i) - \mathbf{x}_i^\top \hat{\beta} \Big)^2 
+ \sum_{i=1}^{n} \log \dot{g}_\alpha(y_i) - n \log(\hat{\sigma})
$$

Reduced by substituting $RSS_g = \sum_{i=1}^{n} \Big( g_\alpha(y_i) - \mathbf{x}_i^\top \hat{\beta} \Big)^2$ and $\hat{\sigma}^2 = \frac{1}{n} RSS_g$:

$$
-\frac{n}{2} \log(2\pi) - \frac{n}{2} + \sum_{i=1}^{n} \log \dot{g}_\alpha(y_i) - n \log(\sqrt{\frac{RSS_g}{n}})
$$

And further into:

$$
l(\alpha; y_1, \dots, y_n) 
= \sum_{i=1}^{n} \log \dot{g}_\alpha(y_i) - \frac{n}{2} \log(\frac{RSS_g}{n}) + const.
$$

\nextpart

I decided to write $g_\alpha$ and $\dot{g}_\alpha$ in such way so that they can be aplied vector-wise and not element-wise.
Implementation of all the functions in R looks as follows:

```{r}
#| message: false
#| warning: false

g = function(y, alpha) {
  out <- numeric(length(y))

  # case 1: y >= 0, alpha != 0
  in_1 = (y >= 0 & alpha != 0)
  out[in_1] = (((y[in_1] + 1)^(alpha - 1)) / alpha)

  # case 2: y >= 0, alpha == 0
  in_2 = (y >= 0 & alpha == 0)
  out[in_2] = log(y[in_2] + 1)

  # case 3: y < 0, alpha != 2
  in_3 = (y < 0 & alpha != 2)
  out[in_3] = ( - ((((-y[in_3] + 1)^(2 - alpha)) - 1) / (2 - alpha)))

  # case 4: y < 0, alpha == 2
  in_4 = (y < 0 & alpha == 2)
  out[in_4] = ( - log(-y[in_4] + 1))

  return(out)
}
```

\newpage

```{r}
g_prim = function(y, alpha) {
  out <- numeric(length(y))

  # case 1: y >= 0
  in_1 = (y >= 0)
  out[in_1] = (y[in_1] + 1)^(alpha - 1)

  # case 2: y < 0
  in_2 = (y < 0)
  out[in_2] = (1 - y[in_2])^(1 - alpha)

  return(out)
}

profile_log_likelihood = function(Y, X, alpha) {
  n = length(Y)

  # transformed Y
  g_Y <- g(Y, alpha)

  # B_hat estimate
  B_hat = solve(t(X) %*% X) %*% t(X) %*% g_Y

  # Residual Sum of Squares
  RSS = sum((g_Y - X %*% B_hat)^2)

  # Constants
  constants = (-n/2)*log(2*pi) - (n/2)

  # profile log likehood = sum of log(g_prim(y)) -n/2 log(sigma_hat^2) + constants
  out = sum(log(g_prim(Y, alpha))) - ((n/2)*log((1/n) * RSS)) + constants

  return(out)
}
```

\nextpart

We then load the data, add intercept term and find the ML estimate using `optim()`
```{r}
#| message: false
#| warning: false
load("Data_Assignment2_Ex2_E2025.rdata")
df_x <- cbind(1, x)
df_y <- Y

out <- optim(
  par = 0,
  fn = function(a) {-profile_log_likelihood(df_y, df_x, a)}
)
```

We plot the values of loglik funciton spread in $\pm$ 0.1 around from `out$par` (which is the result of the optimization) with step of 0.001:

![](plots/Loglik.png){width=70% fig-align="center"}

As we can see $\hat{\alpha} \approx 1.8$

Final transformation is then (by substituting $\alpha = 1.8$):

$$
g_\alpha(Y) =
\begin{cases}
\dfrac{(Y+1)^{1.8} - 1}{1.8}, & y \ge 0\\[6pt]
-\dfrac{(-Y+1)^{0.2} - 1}{0.2}, & y < 0\\[6pt]
\end{cases}
$$
